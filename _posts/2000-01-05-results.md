---
title: "Results"
bg: white
color: black
style: center
fa-icon: line-chart
---

The following WandB reports provide more detailed results than those included in the extended abstract.

We experiment with *(i)* a custom "toy" example Minecraft map; a *(ii)* realistic Minecraft map; and a *(iii)* Habitat apartment. We test the performance of *contrastive* (CURL-ATC) and *reconstruction* (VQ-VAE) approaches for learning meninful representations of the image observations in a self-supervised manner. Then, we apply a clustering of the embedding space to obtain a finite set of representatives of the representations. In the following reports we show the capabilities of both models for providing valuable features for RL agents. The plots show that the image observations embeddings encode both existing similarities within the state space and spatial relations in the 3D environment. In this way, we provide a framework for empowering agents to discover task-agnostic state-covering skills.


### [Discovered skills](https://wandb.ai/embodied-rl-agents/cvpr-workshop/reports/PixelEDL-results-comparison-Index-maps--Vmlldzo2NzYyMDA)

<img src="https://github.com/imatge-upc/PixelEDL/blob/gh-pages/assets/results-IndexMaps.png?raw=true" width=500><br>

<br>
<br>

### [Reward distribution at the Learning stage](https://wandb.ai/embodied-rl-agents/cvpr-workshop/reports/PixelEDL-results-comparison-Reward-maps--Vmlldzo2NzYyMTM)

#### CURL
<img src="https://github.com/imatge-upc/PixelEDL/blob/gh-pages/assets/habitat-contrastive.png?raw=true" width=500><br>
<img src="https://github.com/imatge-upc/PixelEDL/blob/gh-pages/assets/minecraft-contrastive.png?raw=true" width=500><br>

#### VQ-VAE
<img src="https://github.com/imatge-upc/PixelEDL/blob/gh-pages/assets/habitat-reconstruction.png?raw=true" width=500><br>
<img src="https://github.com/imatge-upc/PixelEDL/blob/gh-pages/assets/minecraft-reconstruction.png?raw=true" width=500><br>
